---
title: "[TITLE]"
subtitle: "[SUBTITLE]" 
author: "Joey Hotz"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  bookdown::html_document2:
    theme: readable
    highlight: tango
    number_sections: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import-packages, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(dtplyr)
library(dplyr)
library(data.table)
library(knitr)
library(kableExtra)
library(httr)
library(xml2)
library(readr)
library(stringr)
library(wordcloud)
library(wesanderson)
library(leaflet)
```

# Using the NCBI API

In this project, we will scrape data from the NCBI (National Center for Biotechnology Information) website using the [NCBI API](https://www.ncbi.nlm.nih.gov/home/develop/api/). In particular, the data which we will scrape corresponds to papers relating to COVID vaccines and vaccinations.

## Scraping the NCBI Website

The papers which we are collecting using the API are available on the [PubMed website](https://pubmed.ncbi.nlm.nih.gov/), and the search term we are using to search for papers is `"sars-cov-2 vaccine"`.

```{r scrape-site-1, echo = FALSE}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+vaccine")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex and converting it to a numeric 
counts_num <- stringr::str_extract(counts, "[0-9,]+") %>%
  stringr::str_remove_all(",") %>%
  strtoi()
```

Using the `xml2` package in R, we were able to scrape the PubMed website to find papers containing the phrase `"sars-cov-2 vaccine"`. In total, the PubMed website had `r counts_num` papers matching this query.

Next, we will retrieve information corresponding to these `r counts_num` papers. To do this, we will retrieve the IDs corresponding to these papers on the PubMed website, and we will then use these IDs (alongside the NCBI API) to collect information about the papers themselves. We will only retrieve the first 250 papers' IDs from the PubMed website, to ensure that we are not overloading their website with requests.

```{r get-paper-ids, echo = FALSE}
query_ids <- GET(url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
                 query = list(db = "pubmed",
                              term = "sars-cov-2 vaccine",
                              retmax = 250))

# Extracting the content of the response of GET
ids <- httr::content(query_ids)

# Turn the result into a character vector
ids <- as.character(ids)

# Find all the ids 
ids <- unlist(stringr::str_extract_all(ids, "<Id>[0-9]+</Id>")) 

# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
```

## Extracting Data

Now, we will use the 250 PubMed IDs which we have collected to create a dataset containing the following information for each paper:

* PubMed ID number
* Title of the paper
* Name of the journal where it was published
* Publication date
* Abstract of the paper

We have already collected the PubMed IDs above, as these IDs are necessary in order to requests and extract the remaining information using the NCBI API. 

To determine the remaining four values for each of the 250 papers, we will use RegEx patterns to extract these particular bits of information from the HTML code which we have scraped from the NCBI website.


```{r get-papers, echo = FALSE}
publications <- GET(url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
                    query = list(db = "pubmed",
                                 id = paste(ids, collapse = ","),
                                 retmax = 250,
                                 rettype = "abstract"))

# Turning the output into character vector
publications <- httr::content(publications)
```

The text which we have downloaded by scraping from the NCBI website contains a total of `r nchar(as.character(publications))` characters. However, this text includes many characters which are unrelated to the information we need, as it was read straight from the code for the webpage.

To extract the information we need for creating this database, we will first convert need to break the large chunk of data for all 250 papers into 250 smaller chunks; one representing each individual paper.

After this, we will convert the XML and HTML code for these 250 papers into text, which we will be able to parse and extract information from in a more structured manner.

```{r get-publications-list, echo = FALSE}
publications_list <- xml2::xml_children(publications)
publications_list <- sapply(publications_list, as.character)
```

Next, for each of the 250 papers which we have selected using the NCBI API, we will find the paper's title, the name of the journal it was published in, its publication date, and the abstract of the paper.

```{r get-paper-titles, echo = FALSE}
# Collects titles of the papers 
paper_titles <- stringr::str_extract(publications_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")

# Removes any XML/HTML tags from the text
paper_titles <- stringr::str_remove_all(paper_titles, "<(.+?)>")

# Removes newline characters
paper_titles <- stringr::str_remove_all(paper_titles, "\\n")

# Turns instances of multiple spaces into only one space character
paper_titles <- stringr::str_replace_all(paper_titles, "\\s+", " ")

# Remove leading and trailing whitespace
paper_titles <- stringr::str_remove(paper_titles, "^\\s+")
paper_titles <- stringr::str_remove(paper_titles, "\\s+$")
```

```{r get-journal-names, echo = FALSE}
# Collects all text in the journal section of the HTML code (journal name, publication date, etc)
journal_names <- stringr::str_extract(publications_list, "<Journal>(\\n|.)+</Journal>")

# Selects the title of the journal
journal_names <- stringr::str_extract(journal_names, "<Title>(\\n|.)+</Title>")

# Removes any XML/HTML tags from the text
journal_names <- stringr::str_remove_all(journal_names, "<(.+?)>")

# Removes newline characters
journal_names <- stringr::str_remove_all(journal_names, "\\n")

# Turns instances of multiple spaces into only one space character
journal_names <- stringr::str_replace_all(journal_names, "\\s+", " ")

# Remove leading and trailing whitespace
journal_names <- stringr::str_remove(journal_names, "^\\s+")
journal_names <- stringr::str_remove(journal_names, "\\s+$")
```


```{r get-abstracts, echo = FALSE}
# Collects all text in the abstracts (extracts text, labels, backgrounds, etc)
abstracts <- stringr::str_extract(publications_list, "<Abstract>(\\n|.)+</Abstract>")

# Removes any XML/HTML tags from the text
abstracts <- stringr::str_remove_all(abstracts, "<(.+?)>")

# Removes newline characters
abstracts <- stringr::str_remove_all(abstracts, "\\n")

# Turns instances of multiple spaces into only one space character
abstracts <- stringr::str_replace_all(abstracts, "\\s+", " ")

# Remove leading and trailing whitespace
abstracts <- stringr::str_remove(abstracts, "^\\s+")
abstracts <- stringr::str_remove(abstracts, "\\s+$")
```

Of these 250 papers, `r sum(is.na(stringr::str_extract(publications_list, "<Abstract>(\\n|.)+</Abstract>")))` do not appear to have an abstract.

## Creating the Dataset

Lastly, we will combine the information about our 250 papers which we have collected into a dataset, containing the collected information for each of these 250 papers.

# Text Mining

The [PubMed dataset](https://github.com/JSC370/jsc370-2022/blob/main/data/text/pubmed.csv) which was published on the course GitHub.

```{r import-pubmed-data, echo = FALSE, message = FALSE, warning = FALSE}
pubmed <- read_csv("pubmed.csv")
```