---
title: "Lab 10 - Trees, Bagging, Random Forest"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Perform classification and regression with tree-based methods in R
- Recognize that tree-based methods are capable of capturing non-linearities by splitting multiple times on the same variables
- Compare the performance of classification trees, bagging,and random forests for predicting heart disease based on the ``heart`` data.

# Lab description

For this lab we will be working with simulated data and the `heart` dataset that you can download from [here](https://github.com/JSC370/jsc370-2022/blob/main/data/heart/heart.csv)


### Setup packages

You should install and load `rpart` (trees), `randomForest` (random forest), `gbm` (gradient boosting) and `xgboost` (extreme gradient boosting).


```{r, eval=FALSE, warning=FALSE}
install.packages(c("rpart","randomForest","gbm","xgboost"))
```

### Load packages and data
```{r, warning=FALSE, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)

heart <- read.csv("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/heart/heart.csv")

knitr::kable(head(heart))
```


---

## Question 1: Trees with simulated data

- Simulate data from a random uniform distribution [-5,5] and normally distributed errors (s.d = 0.5)
- Create a non-linear relationship y=sin(x)+error
- Split the data into test and training sets (500 points each), plot the data

```{r, eval=FALSE, echo=FALSE, warning=FALSE}
set.seed(2001)
n <- 1000

x <- runif(n, -5, 5)
error <- rnorm(n, mean = 0, sd = 0.5)
y <-sin(x) + error
data <- data.frame(y = y, x = x)

sample_indices <- sample(1:1000, size = 500)
train_data <- data[sample_indices, ]
test_data <- data[-sample_indices, ]
ggplot(data, aes(x = x, y = y)) + 
  geom_point() +
  theme_bw()
```

- Fit a regression tree using the training set, plot it

```{r, eval=FALSE, echo=FALSE, warning=FALSE}
treefit <- rpart(y ~ x, method = "anova", control = list(cp = 0), data = train_data)

rpart.plot(treefit)
```

- Determine the optimal complexity parameter (cp) to prune the tree

```{r, eval=FALSE, echo=FALSE, warning=FALSE}
plotcp(treefit)
```

- Prune and plot the tree and summarize

```{r, eval=FALSE, echo=FALSE, warning=FALSE}

```

- Based on the plot and/or summary of the pruned tree create a vector of the (ordered) split points for variable x, and a vector of fitted values for the intervals determined by the split points of x.

```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```
- plot the step function corresponding to the fitted (pruned) tree
```{r, eval=FALSE, echo=FALSE, warning=FALSE}

```

- Fit a linear model to the training data and plot the regression line. 
- Contrast the quality of the fit of the tree model vs. linear regression by inspection of the plot
- Compute the test MSE of the pruned tree and the linear regression model

```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```
- Is the lm or regression tree better at fitting a non-linear function?

---

## Question 2: Analysis of Real Data

- Split the `heart` data into training and testing (70-30%)
```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

- Fit a classification tree using rpart, plot the full tree
```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

- Plot the complexity parameter table for an rpart fit and prune the tree
```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

- Compute the test misclassification error
```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

- Fit the tree with the optimal complexity parameter to the full data (training + testing)
```{r,echo=FALSE, eval=FALSE, warning=FALSE}

```
 - Out of Bag (OOB) error for tree
 
```{r,echo=FALSE, eval=FALSE, warning=FALSE}

```

---

## Question 3: Bagging, Random Forest

- Use the training and testing sets from above. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 
- For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

- For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.

```{r, echo=FALSE, eval=FALSE, warning=FALSE}

```

---

# Question 4: Boosting

- For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 

```{r}


```
---


# Deliverables

1. Questions 1-4 answered, pdf or html output uploaded to quercus
